'''
Dataloader for nuScenes data

'''
from builtins import property
from functools import reduce, partial
import multiprocessing
import os
from pathlib import Path
import pdb
import pickle
import random

import cv2
import gin
from matplotlib import pyplot as plt
from nuscenes import NuScenes
from nuscenes.eval.common.utils import quaternion_yaw
from nuscenes.eval.prediction.splits import get_prediction_challenge_split
from nuscenes.map_expansion.map_api import NuScenesMap
from nuscenes.prediction import PredictHelper
from nuscenes.prediction.helper import convert_local_coords_to_global
from nuscenes.prediction.input_representation.agents import AgentBoxesWithFadedHistory
from nuscenes.prediction.input_representation.combinators import (
    Rasterizer,
    add_foreground_to_image,
)
from nuscenes.prediction.input_representation.interface import InputRepresentation
from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer
from nuscenes.prediction.input_representation.utils import (
    convert_to_pixel_coords,
    get_rotation_matrix,
    get_crops,
)
from pyquaternion.quaternion import Quaternion
from shapely.geometry import Polygon, MultiPolygon, LineString, Point, box

import numpy as np
import tensorflow as tf
import pickle
import shapely.wkt
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
NUSC_LOCATIONS = [
    "singapore-onenorth",
    "singapore-hollandvillage",
    "singapore-queenstown",
    "boston-seaport",
]

NUSC_FREQUENCY_IN_HZ = 2

NUSC_LOCATION_TESTS = NUSC_LOCATIONS + [NUSC_LOCATIONS] + [["singapore-onenorth", "singapore-hollandvillage", "singapore-queenstown"]]


def read_cached_parameterized(filename_fn, shape_fn, dtype=np.float32):
    """
    Generic caching wrapper, using Numpy MEM and pickle files. Caches the output to filename_fn, assuming the name is unique.
    Pickle used for object dtype, mem for everything else.

    :param callable filename_fn: Function returning a unique filename, based on instance_token and sample_token
    :param callable shape_fn: Returns a shape tuple, based on self. May return None for object dtype.
    :param type dtype: dtype of the function result
    :return: The data generated by the wrapped function
    :rtype: np.ndarray or object
    """

    def read_cached(create_fn):
        def inner(self, **kwargs):
            filename = filename_fn(self, **kwargs)
            shape = shape_fn(self)
            file = (
                self.cache_dir / f'{filename}.pkl'
                if dtype == object
                else self.cache_dir / f'{filename}.mem'
            )
            if self.cache_dir is None or not file.exists():
                data = create_fn(self, **kwargs)
                if self.cache_dir is not None:
                    if dtype != object:
                        fmem = np.memmap(
                            file,
                            dtype=dtype,
                            mode='w+',
                            shape=shape,
                        )
                        fmem[:] = data.astype(dtype)[:]
                    else:
                        with open(file, 'wb') as filestream:
                            pickle.dump(data, filestream)
            else:
                if dtype != object:
                    data = (
                        np.memmap(
                            file,
                            dtype=dtype,
                            mode='r',
                            shape=shape,
                        )
                        .copy()
                        .astype(dtype)
                    )
                else:
                    with open(file, 'rb') as filestream:
                        data = pickle.load(filestream)
            return data

        return inner

    return read_cached


@gin.configurable
class NuscenesDataset:
    """
    NuScenes dataset with preprocessing pipeline

    :param str split: train, train_val or val
    :oaram str data_dir: dataset root folder
    :param str,None cache_dir: Optional. Caching directory
    :param str data_version: NuScenes dataset version name
    :param int,str limit: Limit in terms of instances of map name
    :param int eps_set: Covernet epsilon value for trajectory set
    :param bool y_all_valid: If True, y will be all valid trajectories, instead of observed
    :param bool multitask: If True, observed and valid trajectories will be returned
    :paran mp_pool: Optional. Multiprocessing pool for reuse
    """

    name = "nuscenes"

    def __init__(
        self,
        split,
        data_dir='D:/nuScenes_mini',
        cache_dir='cache',
        data_version='v1.0-mini',
        limit=-1,
        eps_set=4,
        y_all_valid=False,
        multitask=False,
        reg = False,
        mp_pool=None,
        seed = 42,
    ):
        # tf.data.experimental.enable_debug_mode()

        self.data_dir = data_dir
        self.data_version = data_version
        self._helper = None
        self._mtp_input_representation = None
        self._trajectories_ego = None
        self._maps = None

        self.split = split

        self.token_pairs_full = get_prediction_challenge_split(
            split, dataroot=self.data_dir
        )
        self.token_pairs_full = [pair.split("_") for pair in self.token_pairs_full]

        agent_rasterizer = AgentBoxesWithFadedHistory(
            None,
            seconds_of_history=1,
            meters_ahead=39,
            meters_behind=9,
            meters_left=24,
            meters_right=24,
        )
        self.image_size = (
            int(
                (agent_rasterizer.meters_left + agent_rasterizer.meters_right)
                / agent_rasterizer.resolution
            ),
            int(
                (agent_rasterizer.meters_left + agent_rasterizer.meters_right)
                / agent_rasterizer.resolution
            ),
            3,
        )

        if cache_dir is not None:
            cache_dir = Path(cache_dir)
            if not cache_dir.is_absolute():
                root = Path(os.path.dirname(os.path.realpath(__file__))).parent.parent
                cache_dir = root / cache_dir
            self.cache_dir = cache_dir / f'{self.image_size[0]}_{self.image_size[1]}'

            self.cache_dir.mkdir(parents=True, exist_ok=True)
        else:
            self.cache_dir = None

        self.sec_of_future = 6

        self.eps_set = eps_set
        self.y_all_valid = y_all_valid
        self.multitask = multitask
        self.reg = reg

        self.mp_pool = mp_pool
        self.driveable_polygons = {}
        self.stop_line_polygons = {}
        self.seed = seed
        self.limit = limit

        if isinstance(limit, str) or isinstance(limit, list):
            print("++++++++++++", limit)
            mapping = self.get_map_mappings()
            self.token_pairs = [
                (instance_token, sample_token)
                for instance_token, sample_token in self.token_pairs_full
                if mapping[sample_token] in limit
            ]
        elif isinstance(limit, float) and limit>0.0 and limit<=1.0: # limit given as percentage

            limit = int(limit*len(self.token_pairs_full))

            self.token_pairs = random.Random(seed).sample(self.token_pairs_full, limit)
        elif isinstance(limit, int) and limit > 0 and limit <= len(self.token_pairs_full):

            self.token_pairs = random.Random(seed).sample(self.token_pairs_full, limit)
        else:
            self.token_pairs = self.token_pairs_full

    @staticmethod
    def is_trajectory_drivable(trajectory):
        # This method should be implemented to check if a trajectory is drivable
        # It should use the existing methods and data structures in the NuscenesDataset class
        # Return 1.0 if drivable, 0.0 otherwise
        # You might need to use class variables or a global NuscenesDataset instance to access necessary data
        # Example implementation (you'll need to adapt this to your specific needs):
        instance_token = NuscenesDataset.current_instance_token
        sample_token = NuscenesDataset.current_sample_token
        helper = NuscenesDataset.helper
        maps = NuscenesDataset.maps
        
        map_name = helper.get_map_name_from_sample_token(sample_token)
        
        polygons = NuscenesDataset.driveable_polygons.get(map_name)
        stopPolygons = NuscenesDataset.stop_line_polygons.get(map_name)
        
        starting_annotation = helper.get_sample_annotation(instance_token, sample_token)
        
        trajectory_global = convert_local_coords_to_global(
            trajectory,
            starting_annotation['translation'],
            Quaternion(starting_annotation['rotation'])
        )
        
        for point in trajectory_global:
            if not NuscenesDataset.point_on_polygons(*point, polygons):
                return 0.0
        
        if NuscenesDataset.crosses_stop_line(trajectory_global, stopPolygons):
            return 0.0
        
        return 1.0
    @staticmethod
    def is_possible_trajectory(trajectory, translation, rotation, driveable_polygons,stop_line_polygons):
        """
        Determines if all keypoints of the trajectory are within the polygons area.

        :param list trajectory: List of 2D points
        :param translation: Translation matrix local to global
        :param rotation: Rotation matrix local to global
        :param list: List of polygons
        :return: True, all points within area
        :rtype: bool
        """
        trajectory_global = convert_local_coords_to_global(
            trajectory,
            translation,
            rotation,
        )
        for point in trajectory_global:
            if not NuscenesDataset.point_on_polygons(*point, driveable_polygons):
                return False
        if NuscenesDataset.crosses_stop_line(trajectory_global, stop_line_polygons):
                return False
        return True

    @staticmethod
    def point_on_polygons(x: float, y, polygons):
        """
        Determines if a single point is within the polygons area.

        :param float x: X coordinate
        :param float y: Y coordinate
        :param list: List of polygons
        :return: True, all points within area
        :rtype: bool
        """
        point = Point(x, y)

        for polygon in polygons:
            if point.within(polygon):
                return True
            else:
                pass

        # If nothing is found, return an empty string.
        return False
    def is_point_in_drivable_area(self, x: float, y: float, map_name: str) -> bool:
        """
        Check if a point is within any drivable area of the map.
        """
        if map_name not in self.driveable_polygons:
            nusc_map = self.maps[map_name]
            records = nusc_map.explorer.map_api.drivable_area
            polygons = [
                nusc_map.explorer.extract_polygon(polygon_token)
                for record in records
                for polygon_token in record['polygon_tokens']
            ]
            self.driveable_polygons[map_name] = polygons
        else:
            polygons = self.driveable_polygons[map_name]
        
        point = Point(x, y)
        return any(point.within(polygon) for polygon in polygons)
    
    @staticmethod
    def crosses_stop_line(trajectory, stop_lines):
        """
        Determines if the trajectory crosses any stop line.

        :param list trajectory: List of 2D points
        :param list stop_lines: List of stop lines (as LineString objects)
        :return: True if the trajectory crosses any stop line
        :rtype: bool
        """
        trajectory_line = LineString(trajectory)
        for stop_line in stop_lines:
            if trajectory_line.intersects(stop_line):
                return True
        return False
    
    @read_cached_parameterized(
        lambda self: f'map_mapping_{self.split}',
        shape_fn=lambda self: None,
        dtype=object,
    )
    def get_map_mappings(self):
        """
        Creates a map from sample_token ids to map name

        :return: The mapping
        :rtype: Dict
        """
        sample_tokens = set(
            [sample_token for instance_token, sample_token in self.token_pairs_full]
        )
        map = {
            sample_token: self.helper.get_map_name_from_sample_token(sample_token)
            for sample_token in sample_tokens
        }
        return map

    @property
    def maps(self):
        if not self._maps:
            self._maps = {}
            self._maps['boston-seaport'] = NuScenesMap(
                map_name='boston-seaport', dataroot=self.data_dir
            )
            self._maps['singapore-hollandvillage'] = NuScenesMap(
                map_name='singapore-hollandvillage', dataroot=self.data_dir
            )
            self._maps['singapore-onenorth'] = NuScenesMap(
                map_name='singapore-onenorth', dataroot=self.data_dir
            )
            self._maps['singapore-queenstown'] = NuScenesMap(
                map_name='singapore-queenstown', dataroot=self.data_dir
            )
        return self._maps

    @property
    def trajectories_ego(self):
        if self._trajectories_ego is None:
            root = Path(os.path.dirname(os.path.realpath(__file__))).parent.parent
            with open(root / "data" / "nuscenes" / f'epsilon_{self.eps_set}.pkl', 'rb') as file:
                trajectories = pickle.load(file)
            self._trajectories_ego = np.asarray(trajectories)
        return self._trajectories_ego

    @property
    def helper(self):
        if not self._helper:
            nuscenes = NuScenes(self.data_version, dataroot=self.data_dir)
            self._helper = PredictHelper(nuscenes)
        return self._helper

    @property
    def mtp_input_representation(self):
        if not self._mtp_input_representation:
            static_layer_rasterizer = StaticLayerRasterizer(
                self.helper,
                meters_ahead=39,
                meters_behind=9,
                meters_left=24,
                meters_right=24,
            )
            agent_rasterizer = AgentBoxesWithFadedHistory(
                self.helper,
                seconds_of_history=1,
                meters_ahead=39,
                meters_behind=9,
                meters_left=24,
                meters_right=24,
            )
            self._mtp_input_representation = InputRepresentation(
                static_layer_rasterizer, agent_rasterizer, Rasterizer()
            )
        return self._mtp_input_representation

    @read_cached_parameterized(
        lambda self, instance_token, sample_token: f'{instance_token}_{sample_token}_gt',
        shape_fn=lambda self: (1, self.sec_of_future * NUSC_FREQUENCY_IN_HZ, 2),
    )
    def read_target_trajectories(self, instance_token, sample_token):
        """
        Computes the future trajectory for the ego vehicle

        :param str instance_token: Object instance
        :param str sample_token: Scene sample instance
        :return: The future trajectory in ego coordinates
        :rtype: np.ndarray
        """
        future_for_agent = np.expand_dims(
            self.helper.get_future_for_agent(
                instance_token,
                sample_token,
                self.sec_of_future,
                in_agent_frame=True,
                just_xy=True,
            ),
            0,
        )
        return future_for_agent

    @read_cached_parameterized(
        lambda self, instance_token, sample_token: f'{instance_token}_{sample_token}_{self.eps_set}_possible',
        shape_fn=lambda self: self.trajectories_ego.shape,
    )
    def read_possible_trajectories(self, instance_token, sample_token):
        """
        Computes the all possible trajectories for the ego vehicle, based on driveable area.

        :param str instance_token: Object instance
        :param str sample_token: Scene sample instance
        :return: The future trajectories in ego coordinates. Invalid trajectories have 0 values.
        :rtype: np.ndarray
        """
        map_name = self.helper.get_map_name_from_sample_token(sample_token)
        #for drivable area polygon
        if map_name not in self.driveable_polygons:
            nusc_map = self.maps[map_name]
            records = nusc_map.explorer.map_api.drivable_area
            polygons = [
                nusc_map.explorer.extract_polygon(polygon_token)
                for record in records
                for polygon_token in record['polygon_tokens']
            ]
            self.driveable_polygons[map_name] = polygons
        else:
            polygons = self.driveable_polygons[map_name]
        # for the stop line polygon collection
        if map_name not in self.stop_line_polygons:
            nusc_map = self.maps[map_name]
            stop_records = nusc_map.stop_line
            stopPolygons = [
                nusc_map.explorer.extract_polygon(record['polygon_token'])
                for record in stop_records
                #for polygon_token in record['polygon_tokens']
            ]
            self.stop_line_polygons[map_name] = stopPolygons
        else:
            stopPolygons = self.stop_line_polygons[map_name]
            
        starting_annotation = self.helper.get_sample_annotation(
            instance_token, sample_token
        )

        trajectory_global_start = convert_local_coords_to_global(
            self.trajectories_ego[0][0:1],
            starting_annotation['translation'],
            starting_annotation['rotation'],
        )
        # drivable area polygon sorting
        polygons = sorted(
            polygons,
            key=lambda item: item.distance(
                Point(trajectory_global_start[0, 0], trajectory_global_start[0, 1])
            ),
        )
        stopPolygons = sorted(
            stopPolygons,
            key=lambda item: item.distance(
                Point(trajectory_global_start[0, 0], trajectory_global_start[0, 1])
            ),
        )
        if self.mp_pool is None:
            pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())
        else:
            pool = self.mp_pool
        possible = pool.map(
            partial(
                NuscenesDataset.is_possible_trajectory,
                translation=starting_annotation['translation'],
                rotation=starting_annotation['rotation'],
                driveable_polygons=polygons,
                stop_line_polygons=stopPolygons,
            ),
            self.trajectories_ego,
        )
        if self.mp_pool is None:
            pool.terminate()
        possible_trajectories = (
            self.trajectories_ego
            * np.asarray(possible, dtype=np.float32)[:, np.newaxis, np.newaxis]
        )
        return possible_trajectories

    def check_driveable_trajectory(self, instance_token, sample_token, trajectory):
        map_name = self.helper.get_map_name_from_sample_token(sample_token)

        if map_name not in self.driveable_polygons:
            nusc_map = self.maps[map_name]
            records = nusc_map.explorer.map_api.drivable_area
            polygons = [
                nusc_map.explorer.extract_polygon(polygon_token)
                for record in records
                for polygon_token in record['polygon_tokens']
            ]
            self.driveable_polygons[map_name] = polygons
        else:
            polygons = self.driveable_polygons[map_name]

        if map_name not in self.stop_line_polygons:
            nusc_map = self.maps[map_name]
            stop_records = nusc_map.stop_line
            stopPolygons = [
                nusc_map.explorer.extract_polygon(record['polygon_token']) 
                for record in stop_records
            ]
            self.stop_line_polygons[map_name] = stopPolygons
        else:
            stopPolygons = self.stop_line_polygons[map_name]

        starting_annotation = self.helper.get_sample_annotation(
            instance_token, sample_token
        )

        trajectory_global_start = convert_local_coords_to_global(
            trajectory[0:1],
            starting_annotation['translation'],
            starting_annotation['rotation'],
        )
        
        # sorting drivable area polygons
        polygons = sorted(
            polygons,
            key=lambda item: item.distance(
                Point(trajectory_global_start[0, 0], trajectory_global_start[0, 1])
            ),
        )

        # sorting stop line polygons
        stopPolygons = sorted(
            stopPolygons,
            key=lambda item: item.distance(
                Point(trajectory_global_start[0, 0], trajectory_global_start[0, 1])
            ),
        )

        driveability1 = NuscenesDataset.is_possible_trajectory(
            translation=starting_annotation['translation'], 
            rotation=starting_annotation['rotation'], 
            driveable_polygons=polygons, 
            stop_line_polygons=stopPolygons, 
            trajectory=trajectory
        )

        return driveability1
  
    def check_driveable_trajectory_m(self, instance_token, sample_token, trajectory):
        map_name = self.helper.get_map_name_from_sample_token(sample_token)
        
        if map_name not in self.driveable_polygons:
            nusc_map = self.maps[map_name]
            records = nusc_map.explorer.map_api.drivable_area
            polygons = [
                nusc_map.explorer.extract_polygon(polygon_token)
                for record in records
                for polygon_token in record['polygon_tokens']
            ]
            self.driveable_polygons[map_name] = polygons
        else:
            polygons = self.driveable_polygons[map_name]


        
        starting_annotation = self.helper.get_sample_annotation(instance_token, sample_token)
        
        trajectory_global = convert_local_coords_to_global(
            trajectory,
            starting_annotation['translation'],
            Quaternion(starting_annotation['rotation'])
        )
        
        for point in trajectory_global:
            if not NuscenesDataset.point_on_polygons(*point, polygons):
                return False
        return True
    def check_stop_trajectory_m(self, instance_token, sample_token, trajectory):
        map_name = self.helper.get_map_name_from_sample_token(sample_token)
        
        if map_name not in self.stop_line_polygons:
            nusc_map = self.maps[map_name]
            stop_records = nusc_map.stop_line
            stopPolygons = [
                nusc_map.explorer.extract_polygon(record['polygon_token'])
                for record in stop_records
            ]
            self.stop_line_polygons[map_name] = stopPolygons
        else:
            stopPolygons = self.stop_line_polygons[map_name]
        
        starting_annotation = self.helper.get_sample_annotation(instance_token, sample_token)
        
        trajectory_global = convert_local_coords_to_global(
            trajectory,
            starting_annotation['translation'],
            Quaternion(starting_annotation['rotation'])
        )
        crosses_stop_line = NuscenesDataset.crosses_stop_line(trajectory_global, stopPolygons)
        return not crosses_stop_line

    @read_cached_parameterized(
        lambda self, instance_token, sample_token: f'{instance_token}_{sample_token}',
        shape_fn=lambda self: self.image_size,
        dtype=np.uint8,
    )
    def read_image(self, instance_token, sample_token):
        """
        Computes the MTP/Covernet image representation of the current scene

        :param str instance_token: Object instance
        :param str sample_token: Scene sample instance
        :return: The RBG image
        :rtype: np.ndarray
        """
        image = self.mtp_input_representation.make_input_representation(
            instance_token, sample_token
        )
        return image

    @read_cached_parameterized(
        lambda self, instance_token, sample_token: f'{instance_token}_{sample_token}_state',
        shape_fn=lambda self: (3),
    )
    def read_agent_state(self, instance_token, sample_token):
        """
        Computes the ego state vector with velocity, acceleation and heading

        :param str instance_token: Object instance
        :param str sample_token: Scene sample instance
        :return: The current stae
        :rtype: np.ndarray
        """
        agent_state_vector = np.asarray(
            [
                self.helper.get_velocity_for_agent(instance_token, sample_token),
                self.helper.get_acceleration_for_agent(instance_token, sample_token),
                self.helper.get_heading_change_rate_for_agent(
                    instance_token, sample_token
                ),
            ]
        )
        return agent_state_vector

    def get_dataset(self, mdl_input_names, mdl_output_names, shuffle=False):
        """
        Returns the dataset, as specified by the constructor.

        :param bool shuffle: Dataset is shuffled
        :return: The dataset
        :rtype: tf.data.Dataset
        """
        
        instance_tokens = []
        sample_tokens = []
        for instance_token, sample_token in self.token_pairs:
            instance_tokens.append(instance_token)
            sample_tokens.append(sample_token)

        def read_data(instance_token, sample_token):
            instance_token = instance_token.numpy().decode()
            sample_token = sample_token.numpy().decode()
            
            image = self.read_image(instance_token=instance_token, sample_token=sample_token)
            agent_state_vector = self.read_agent_state(instance_token=instance_token, sample_token=sample_token)
            agent_state_vector[np.isnan(agent_state_vector)] = 0
            future_for_agent_valid = self.read_possible_trajectories(instance_token=instance_token, sample_token=sample_token)
            future_for_agent_observed = self.read_target_trajectories(instance_token=instance_token, sample_token=sample_token)
            
            return image, agent_state_vector, future_for_agent_observed, future_for_agent_valid, instance_token, sample_token


        instance_dataset = tf.data.Dataset.from_tensor_slices(instance_tokens)
        sample_dataset = tf.data.Dataset.from_tensor_slices(sample_tokens)
        dataset = tf.data.Dataset.zip((instance_dataset, sample_dataset))
        if shuffle:
            dataset = dataset.shuffle(len(self.token_pairs))
        
        def project_data(image, agent_state_vector, future_for_agent_observed, future_for_agent_valid,instance_token, sample_token): 
            possible_inputs = {
                'image': image, 
                'state': agent_state_vector,
                'instance_token': instance_token,
                'sample_token': sample_token
            }
            possible_outputs = {
                'cls': future_for_agent_observed, 
                "trajs": future_for_agent_observed,
                'cls_for_label': future_for_agent_valid, 
                'label': future_for_agent_valid, 
                "reg": future_for_agent_observed,
                }   
            inputs = {key: value for key, value in possible_inputs.items() if key in mdl_input_names}
            outputs = {key: value for key, value in possible_outputs.items() if key in mdl_output_names}
            return inputs, outputs

        dataset = dataset.map(
                    lambda instance_token, sample_token: tf.py_function(
                        read_data,
                        inp=[instance_token, sample_token],
                        Tout=[tf.float32, tf.float32, tf.float32, tf.float32,tf.string, tf.string],
                    )
                ).map(project_data)
        return dataset