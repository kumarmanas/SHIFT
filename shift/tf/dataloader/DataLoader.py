'''
General Purpose DataLoader

'''
from builtins import property
from functools import reduce, partial
import multiprocessing
import os
from pathlib import Path
import pdb
import pickle
import random

import cv2
import gin
from matplotlib import pyplot as plt
from nuscenes import NuScenes
from nuscenes.eval.common.utils import quaternion_yaw
from nuscenes.eval.prediction.splits import get_prediction_challenge_split
from nuscenes.map_expansion.map_api import NuScenesMap
from nuscenes.prediction import PredictHelper
from nuscenes.prediction.helper import convert_local_coords_to_global
from nuscenes.prediction.input_representation.agents import AgentBoxesWithFadedHistory
from nuscenes.prediction.input_representation.combinators import (
    Rasterizer,
    add_foreground_to_image,
)
from nuscenes.prediction.input_representation.interface import InputRepresentation
from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer
from nuscenes.prediction.input_representation.utils import (
    convert_to_pixel_coords,
    get_rotation_matrix,
    get_crops,
)
from pyquaternion.quaternion import Quaternion
from shapely.geometry import Polygon, MultiPolygon, LineString, Point, box

import numpy as np
import tensorflow as tf
import pickle


DEFAULT_INPUT_CROP = {
    "meters_ahead": 39,
    "meters_behind": 9,
    "meters_left": 24,
    "meters_right": 24,
}
DEFAULT_RESOLUTION = 0.1
DEFAULT_SEC_OF_HISTORY = 1
DEFAULT_SEC_OF_FUTURE = 6

def read_cached_parameterized(filename_fn, shape_fn, dtype=np.float32):
    """
    Generic caching wrapper, using Numpy MEM and pickle files. Caches the output to filename_fn, assuming the name is unique.
    Pickle used for object dtype, mem for everything else.

    :param callable filename_fn: Function returning a unique filename, based on instance_token and sample_token
    :param callable shape_fn: Returns a shape tuple, based on self. May return None for object dtype.
    :param type dtype: dtype of the function result
    :return: The data generated by the wrapped function
    :rtype: np.ndarray or object
    """

    def read_cached(create_fn):
        def inner(self, **kwargs):
            filename = filename_fn(self, **kwargs)
            shape = shape_fn(self)
            if self.cache_dir is not None:
                file = (
                    self.cache_dir / f'{filename}.pkl'
                    if dtype == object
                    else self.cache_dir / f'{filename}.mem'
                )
            else:
                file = (
                    f'{filename}.pkl'
                    if dtype == object
                    else f'{filename}.mem'
                )
            if self.cache_dir is None or not file.exists():
                data = create_fn(self, **kwargs)
                if self.cache_dir is not None:
                    if dtype != object:
                        fmem = np.memmap(
                            file,
                            dtype=dtype,
                            mode='w+',
                            shape=shape,
                        )
                        fmem[:] = data.astype(dtype)[:]
                    else:
                        with open(file, 'wb') as filestream:
                            pickle.dump(data, filestream)
            else:
                if dtype != object:
                    data = (
                        np.memmap(
                            file,
                            dtype=dtype,
                            mode='r',
                            shape=shape,
                        )
                        .copy()
                        .astype(dtype)
                    )
                else:
                    with open(file, 'rb') as filestream:
                        data = pickle.load(filestream)
            return data

        return inner

    return read_cached


class DataLoader():
    """
    General Dataloader class for CoverNet/MultiPath models

    :param str split: train, train_val or val
    :oaram str data_dir: dataset root folder
    :param str,None cache_dir: Optional. Caching directory
    :param str data_version: NuScenes dataset version name
    :param int,str limit: Limit in terms of instances of map name
    :param int eps_set: Covernet epsilon value for trajectory set
    :param bool multitask: If True, observed and valid trajectories will be returned
    :paran mp_pool: Optional. Multiprocessing pool for reuse
    """

    def __init__(
        self,
        split,
        data_version,
        data_dir,
        cache_dir = 'cache',
        limit = -1,
        eps_set = 4,
        mp_pool = None,
        seed = 42,
        input_crop = DEFAULT_INPUT_CROP,
        resolution = DEFAULT_RESOLUTION,
        sec_of_history = DEFAULT_SEC_OF_HISTORY,
        sec_of_future = DEFAULT_SEC_OF_FUTURE,
    ):
        self.data_dir = data_dir
        self.data_version = data_version
        self._helper = None
        self._input_representation = None
        self._trajectories_ego = None
        self._maps = None

        self.split = split
        self.sec_of_future = sec_of_future
        self.sec_of_history = sec_of_history
        self.resolution = resolution
        self.input_crop = input_crop

        token_pairs_full = self.load_token_pairs(split)

        self.image_size = (
            int(
                (input_crop["meters_ahead"] + input_crop["meters_behind"])
                / resolution
            ),
            int(
                (input_crop["meters_left"] + input_crop["meters_right"])
                / resolution
            ),
            3, # three channels for RGB Image
        )

        if cache_dir is not None:
            cache_dir = Path(cache_dir)
            if not cache_dir.is_absolute():
                root = Path(os.path.dirname(os.path.realpath(__file__))).parent.parent
                cache_dir = root / cache_dir
            self.cache_dir = cache_dir / f'{self.image_size[0]}_{self.image_size[1]}'

            self.cache_dir.mkdir(parents=True, exist_ok=True)
        else:
            self.cache_dir = None

        self.eps_set = eps_set
        self.mp_pool = mp_pool
        self.seed = seed
        self.driveable_polygons = {}
        self.limit = limit

        if isinstance(limit, str) or isinstance(limit, list):
            mapping = self.get_map_mappings(token_pairs_full)
            self.token_pairs = [
                (instance_token, sample_token)
                for instance_token, sample_token in token_pairs_full
                if mapping[sample_token] in limit
            ]
        elif isinstance(limit, float) and limit>0.0 and limit<=1.0: # limit given as float percentage
            limit = int(limit*len(token_pairs_full))
            self.token_pairs = random.Random(seed).sample(token_pairs_full, limit)
        elif isinstance(limit, int) and limit > 0 and limit <= len(token_pairs_full):
            self.token_pairs = random.Random(seed).sample(token_pairs_full, limit)
        else:
            self.token_pairs = token_pairs_full

    @property
    def maps(self):
        raise NotImplementedError()
    
    @property
    def trajectories_ego(self):
        raise NotImplementedError()

    @property
    def helper(self):
        raise NotImplementedError()
    
    @property
    def input_representation(self):
        raise NotImplementedError()

    @staticmethod
    def is_possible_trajectory(trajectory, translation, rotation, polygons):
        """
        Determines if all keypoints of the trajectory are within the polygons area.

        :param list trajectory: List of 2D points
        :param translation: Translation matrix local to global
        :param rotation: Rotation matrix local to global
        :param list: List of polygons
        :return: True, all points within area
        :rtype: bool
        """
        trajectory_global = convert_local_coords_to_global(
            trajectory,
            translation,
            rotation,
        )
        for point in trajectory_global:
            if not DataLoader.point_on_polygons(*point, polygons):
                return False
        return True

    @staticmethod
    def point_on_polygons(x: float, y, polygons):
        """
        Determines if a single point is within the polygons area.

        :param float x: X coordinate
        :param float y: Y coordinate
        :param list: List of polygons
        :return: True, all points within area
        :rtype: bool
        """
        point = Point(x, y)

        for polygon in polygons:
            if point.within(polygon):
                return True
            else:
                pass

        # If nothing is found, return an empty string.
        return False
    
    def load_token_pairs(self, split):
        """"
        Gets all token pairs for the dataset
        """
        raise NotImplementedError()

    @read_cached_parameterized(
        lambda self: f'map_mapping_{self.split}',
        shape_fn=lambda self: None,
        dtype=object,
    )
    def get_map_mappings(self, token_pairs):
        """
        Creates a map from sample_token ids to map name

        :return: The mapping
        :rtype: Dict
        """
        sample_tokens = set(
            [sample_token for instance_token, sample_token in token_pairs]
        )
        map = {
            sample_token: self.helper.get_map_name_from_sample_token(sample_token)
            for sample_token in sample_tokens
        }
        return map

    def read_possible_trajectories(self, instance_token, sample_token):
        """
        Computes the all possible trajectories for the ego vehicle, based on driveable area.
        """
        raise NotImplementedError()

    def check_driveable_trajectory(self, instance_token, sample_token, trajectory):
        """
        Checks if given arbitrary trajectory is drivable
        """
        raise NotImplementedError()
    
    def read_target_trajectories(self, instance_token, sample_token):
        """
        Computes the future trajectory for the ego vehicle
        """
        raise NotImplementedError()

    @read_cached_parameterized(
        lambda self, instance_token, sample_token: f'{instance_token}_{sample_token}',
        shape_fn=lambda self: self.image_size,
        dtype=np.uint8,
    )
    def read_image(self, instance_token, sample_token):
        """
        Computes the MTP/Covernet image representation of the current scene

        :param str instance_token: Object instance
        :param str sample_token: Scene sample instance
        :return: The RBG image
        :rtype: np.ndarray
        """
        image = self.input_representation.make_input_representation(
            instance_token=instance_token, sample_token=sample_token
        )
        return image

    @read_cached_parameterized(
        lambda self, instance_token, sample_token: f'{instance_token}_{sample_token}_state',
        shape_fn=lambda self: (3),
    )
    def read_agent_state(self, instance_token, sample_token):
        """
        Computes the ego state vector with velocity, acceleation and heading

        :param str instance_token: Object instance
        :param str sample_token: Scene sample instance
        :return: The current stae
        :rtype: np.ndarray
        """
        agent_state_vector = np.asarray(
            [
                self.helper.get_velocity_for_agent(instance_token=instance_token, sample_token=sample_token),
                self.helper.get_acceleration_for_agent(instance_token=instance_token, sample_token=sample_token),
                self.helper.get_heading_change_rate_for_agent(
                    instance_token=instance_token, sample_token=sample_token
                ),
            ]
        )
        return agent_state_vector

    def get_dataset(self, mdl_input_names, mdl_output_names, shuffle=False):
        """
        Returns the dataset, as specified by the constructor.

        :param bool shuffle: Dataset is shuffled
        :return: The dataset
        :rtype: tf.data.Dataset
        """
        
        instance_tokens = []
        sample_tokens = []
        for instance_token, sample_token in self.token_pairs:
            instance_tokens.append(instance_token)
            sample_tokens.append(sample_token)

        def read_data(instance_token, sample_token):
            instance_token = instance_token.numpy().decode()
            sample_token = sample_token.numpy().decode()
            # pydevd.settrace(suspend=True)
            image = self.read_image(
                instance_token=instance_token, sample_token=sample_token
            )
            agent_state_vector = self.read_agent_state(
                instance_token=instance_token, sample_token=sample_token
            )
            agent_state_vector[np.isnan(agent_state_vector)] = 0
            future_for_agent_valid = self.read_possible_trajectories(
                instance_token=instance_token, sample_token=sample_token
            )
            future_for_agent_observed = self.read_target_trajectories(
                instance_token=instance_token, sample_token=sample_token
            )
            return image, agent_state_vector, future_for_agent_observed, future_for_agent_valid

        instance_dataset = tf.data.Dataset.from_tensor_slices(instance_tokens)
        sample_dataset = tf.data.Dataset.from_tensor_slices(sample_tokens)
        dataset = tf.data.Dataset.zip((instance_dataset, sample_dataset))
        if shuffle:
            dataset = dataset.shuffle(len(self.token_pairs))
        
        def project_data(image, agent_state_vector, future_for_agent_observed, future_for_agent_valid): 
            possible_inputs = {'image': image, 'state': agent_state_vector}
            possible_outputs = {
                'cls': future_for_agent_observed, 
                "trajs": future_for_agent_observed,
                'cls_for_label': future_for_agent_valid, 
                'label': future_for_agent_valid, 
                "reg": future_for_agent_observed,
                }   
            inputs = {key: value for key, value in possible_inputs.items() if key in mdl_input_names}
            outputs = {key: value for key, value in possible_outputs.items() if key in mdl_output_names}
            return inputs, outputs

        dataset = dataset.map(
                    lambda instance_token, sample_token: tf.py_function(
                        read_data,
                        inp=[instance_token, sample_token],
                        Tout=[tf.float32, tf.float32, tf.float32, tf.float32],
                    )
                ).map(project_data)
        return dataset